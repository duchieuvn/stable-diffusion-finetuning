\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[tc]{titlepic}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{float}
\usepackage{amsmath}
\usepackage[none]{hyphenat}
\usepackage[
  backend=biber,
  style=ieee,
  sorting=none
    ]{biblatex}
\usepackage[
  a4paper,
  left=2.5cm,
  right=2.5cm,
  top=2.5cm,
  bottom=2.5cm
]{geometry}
\usepackage{indentfirst}

\addbibresource{./assets/literature.bib}
\begin{document}

% Title and author
\begin{titlepage}
  \begin{center}
    \vspace*{2cm}

    
    \includegraphics[width = 10cm]{assets/oth.png}

    \begin{center}
      {Ostbayerische Technische Hochschule Amberg-Weiden}\\
      {Department of Electrical Engineering, Media and Computer Science}
    \end{center}

    \vspace{1.5cm}
    
    {\fontsize{20}{80}\selectfont 
        \bfseries 
        Personalized Image Generation with Stable Diffusion: \\[6pt]
        A PyTorch-Based LoRA Fine-Tuning Approach \\[6pt]
    }

    \vspace{1.5cm}

    \vspace{2cm}

    {\small\ Student}

    \vspace{0.25cm}

    {\Large\bfseries Tran Thai Duc Hieu }
    \vspace{0.25cm}
    
    {\small t.tran@oth-aw.de }

    \vspace{1cm}

    {\small\ Supervised by}

    \vspace{0.25cm}

    {\Large\bfseries Prof. Dr. Tatyana Ivanovska}

    \vspace{2cm}

    
    \vfill

    {\large\today}
  \end{center}
\end{titlepage}


\newpage

\section{Introduction}

Recent advances in diffusion models, particularly Stable Diffusion \\cite{rombach2022high}, have demonstrated remarkable capabilities in text-to-image generation. However, generating highly personalized images of specific subjects or celebrities remains challenging, as it requires the model to learn and preserve identity-specific features while remaining generalizable to diverse prompts and contexts.

Traditional approaches to this problem have relied on UI-based tools such as Kohya and A1111 WebUI, which abstract away the underlying PyTorch implementation details. While these tools are user-friendly, they limit customization and reproducibility for research purposes.

This project aims to achieve personalized image generation performance comparable to existing tutorials and tools, but using pure PyTorch code. By implementing Low-Rank Adaptation (LoRA) fine-tuning from scratch, we provide a transparent, modular, and extensible framework for personalization that facilitates both research and experimentation.

The motivation for this work is threefold:
\begin{enumerate}
  \item To provide a reproducible, open-source PyTorch implementation for Stable Diffusion personalization
  \item To systematically study the impact of caption quality on model performance
  \item To establish evaluation metrics (LPIPS and FID) for quantifying generation quality and identity preservation
\end{enumerate}

\section{Related Work}

\subsection{Diffusion Models and Stable Diffusion}

Diffusion models have emerged as a powerful class of generative models, achieving state-of-the-art results in image synthesis. Stable Diffusion \\cite{rombach2022high} builds upon this foundation by combining diffusion models with latent space representations, enabling efficient high-quality image generation. The architecture comprises three main components: a text encoder (CLIP), a UNet-based denoising network, and a variational autoencoder (VAE).

\subsection{Low-Rank Adaptation (LoRA)}

Full model fine-tuning requires updating billions of parameters, which is computationally expensive and prone to overfitting on small datasets. Low-Rank Adaptation (LoRA) \\cite{hu2021lora} addresses this by introducing trainable low-rank decomposition matrices into the network. Instead of updating the weight matrix $W$, LoRA decomposes the weight update as:

\[
\Delta W = BA^T
\]

where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{k \times r}$, with $r \ll \min(d, k)$ (typically $r=8$ or $r=16$). This dramatically reduces the number of trainable parameters while maintaining model expressiveness. LoRA is particularly effective for Stable Diffusion personalization, where only the attention layers in the UNet are typically adapted.

\subsection{Personalized Image Generation}

Personalized image generation aims to teach the model to recognize and generate specific subjects (e.g., a particular person or object). Common approaches include:

\begin{itemize}
  \item \textbf{Textual Inversion:} Learns a new token embedding to represent a subject, requiring only 3-5 reference images but producing less flexible results.
  \item \textbf{DreamBooth:} Uses subject-specific identifiers combined with class-preservation loss to achieve high-fidelity personalization.
  \item \textbf{LoRA Fine-Tuning:} Adapts attention layers with low-rank updates, balancing efficiency and quality. This approach is implemented in popular community tools like Kohya and A1111 WebUI.
\end{itemize}

\subsection{Impact of Caption Quality on Model Performance}

Recent community research emphasizes the critical role of caption quality in personalization outcomes. The caption strategy directly affects both identity preservation and generation diversity:

\begin{enumerate}
  \item \textbf{Generic captions (v1):} Using simple, fixed identifiers (e.g., ``J3NN13 person'') for all training images. This approach is fastest to prepare but may limit generation diversity and potentially reduce model generalization.
  \item \textbf{Detailed captions (v2):} Rich descriptions including appearance (hair, facial features), clothing, pose, background context, distance to camera, and shot angle. This approach provides maximum diversity but requires significant manual annotation effort.
  \item \textbf{General captions (v3):} Moderate descriptions providing balanced detail and diversity without excessive labeling effort. Often considered a practical middle ground.
\end{enumerate}

\subsection{Evaluation Metrics for Personalized Generation}

Quantitatively evaluating personalized image generation requires metrics that capture both identity preservation and overall synthesis quality:

\begin{itemize}
  \item \textbf{LPIPS (Learned Perceptual Image Patch Similarity):} Measures perceptual similarity between generated and training reference images using features from a pre-trained deep network (typically AlexNet). Lower values indicate better identity preservation. Benchmark targets: $< 0.15$ (excellent), $0.15-0.25$ (good), $> 0.25$ (weak).
  \item \textbf{FID (Fr\\'echet Inception Distance):} Measures statistical distance between generated and reference image feature distributions using the Inception-v3 network. Lower values indicate better overall generation quality and diversity. Benchmark targets: $< 10$ (excellent), $10-30$ (good), $30-50$ (moderate), $> 50$ (poor).
\end{itemize}

\subsection{Community Tools and Motivation for PyTorch Implementation}

UI-based tools like Kohya and A1111 WebUI have democratized LoRA fine-tuning by providing user-friendly interfaces and pre-configured settings. However, these tools often obscure implementation details, limiting research flexibility and code reproducibility. This work implements an equivalent PyTorch-based pipeline, following established community configurations while providing fully interpretable, modular code suitable for research and experimentation.

\section{Dataset Preparation and Experimental Setup}

\subsection{Data Collection and Preprocessing}

For this study, we compiled a dataset of celebrity images (Jennie) to evaluate personalized generation. The dataset preparation process involved three steps:

\begin{enumerate}
  \item \textbf{Image Collection:} Downloaded high-quality reference images of the target subject from public sources.
  
  \item \textbf{Image Preprocessing:} All images were resized to $512 \times 512$ pixels while preserving the original aspect ratio to maintain quality. Images with excessive occlusion or low quality were manually removed.
  
  \item \textbf{Image Captioning:} Images were annotated using three distinct strategies to evaluate the impact of caption quality on model performance (detailed below).
\end{enumerate}

\subsection{Caption Strategies and Dataset Versions}

To systematically study the effect of caption quality, we created three dataset versions with different captioning approaches:

\begin{itemize}
  \item \textbf{Version 1 (Generic captions, v1):} All images labeled with a simple identifier ``J3NN13 person''. This represents the minimal caption strategy, testing whether a fixed token can capture subject identity.
  
  \item \textbf{Version 2 (Detailed captions, v2):} Rich manual descriptions including appearance (hair color/style, facial features), clothing, pose, background context, camera distance (close-up, medium, full-body), and shot angle (frontal, profile, three-quarter). Expected to maximize generation diversity.
  
  \item \textbf{Version 3 (General captions, v3):} Moderate-length descriptions providing balanced detail (e.g., ``J3NN13 person wearing blue shirt in a bright room, medium shot'') without exhaustive annotation. Intended as a practical compromise.
\end{itemize}


\section{Experimental Setup and Implementation}

\subsection{Model and Training Configuration}

All experiments utilize the Stable Diffusion v1.5 base model (pretrained on LAION-5B), which provides a strong foundation for personalization. We fine-tune only the attention layers of the UNet denoising network using LoRA with the following configuration:

\begin{table}[H]
  \centering
  \caption{LoRA Fine-Tuning Hyperparameters}
  \label{tab:hyperparams}
  \begin{tabular}{@{}lcc@{}}
    \toprule
    Parameter & Value & Notes \\
    \midrule
    Learning Rate & $1 \times 10^{-4}$ & Standard for LoRA fine-tuning \\
    Batch Size & 4-8 & GPU memory constrained \\
    Training Steps & 1000-2000 & Varies with dataset size \\
    LoRA Rank ($r$) & 8 or 16 & Low-rank decomposition dimension \\
    LoRA Alpha & 16 & Scaling factor for LoRA contribution \\
    Optimizer & AdamW & Standard choice with $\beta_1=0.9, \beta_2=0.999$ \\
    Scheduler & Linear & Learning rate decay to zero \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Training Implementation Details}

All models were implemented in PyTorch using the Hugging Face \texttt{diffusers} library and trained on NVIDIA GPUs. The training pipeline ensures comparability across experiments:

\begin{itemize}
  \item \textbf{Data Loading:} Images and captions are loaded via custom PyTorch Dataset class with random augmentations (random crops, horizontal flips).
  \item \textbf{Loss Function:} Mean Squared Error (MSE) between predicted and actual noise in the latent space, standard for diffusion model training.
  \item \textbf{Validation:} Every 100 steps, we generate sample images using learned prompts (e.g., ``photo of J3NN13 person wearing blue shirt'') to monitor training progress.
  \item \textbf{Model Checkpointing:} LoRA weights saved at epochs 6, 8, 10, 12 to identify optimal checkpoint.
\end{itemize}


\subsection{Evaluation Metrics for Personalized Generation}

We evaluate the quality of personalized image generation using two complementary perceptual metrics:

\begin{itemize}

  \item \textbf{LPIPS (Learned Perceptual Image Patch Similarity):}  
  LPIPS measures perceptual similarity between generated and reference training images by computing distances in the feature space of a pre-trained deep network (we use AlexNet). It is computed as:
  \[
  \text{LPIPS}(x, y) = \sum_l \frac{1}{H_l W_l} \sum_{h,w} \left\| w_l \odot (f_l^{(x)}(h,w) - f_l^{(y)}(h,w)) \right\|_2^2
  \]
  where $f_l$ denotes layer activations, $w_l$ are learned weights, and $\odot$ is element-wise multiplication. Lower scores indicate better identity preservation (target: $< 0.20$ for excellent results).

  \item \textbf{FID (Fr\\'echet Inception Distance):}  
  FID measures the statistical distance between feature distributions of generated and real reference images using the Inception-v3 network:
  \[
  \text{FID} = \left\| \mu_x - \mu_y \right\|_2^2 + \text{Tr}\left(\Sigma_x + \Sigma_y - 2(\Sigma_x \Sigma_y)^{1/2}\right)
  \]
  where $\mu$ and $\Sigma$ denote mean and covariance of features. Lower scores indicate better overall generation quality and diversity (target: $< 30$ for good results).

  \item \textbf{Visual Inspection:}  
  We qualitatively assess generated images for identity consistency, detail preservation, and diversity across different prompt variations.

\end{itemize}

\subsection{Evaluation Protocol}

For each trained LoRA checkpoint, we:
\begin{enumerate}
  \item Generate 50-100 diverse images using varied prompts containing the identity token
  \item Compare generated images to training reference images using LPIPS
  \item Compute FID against the full reference dataset
  \item Identify the best-performing checkpoint based on metric scores
  \item Visually inspect generated samples for quality and artifacts
\end{enumerate}



\subsection{Results and Analysis}

We present results from fine-tuning LoRA models on three caption strategy variants (v1: generic, v2: detailed, v3: general). Each model was evaluated using LPIPS for identity preservation and FID for overall generation quality.

\subsubsection{Quantitative Results: LPIPS and FID Scores}

\begin{table}[H]
  \centering
  \caption{Evaluation metrics for Jennie dataset across caption strategies}
  \label{tab:results-summary}
  \begin{tabular}{@{}lcccc@{}}
    \toprule
    Model Version & Caption Type & LPIPS & FID & Quality Assessment \\
    \midrule
    v1 & Generic (``J3NN13 person'') & 0.284 & 42.3 & Moderate preservation \\
    v2 & Detailed (varied descriptions) & 0.156 & 18.7 & Excellent preservation \\
    v3 & General (moderate detail) & 0.201 & 26.5 & Good preservation \\
    \bottomrule
  \end{tabular}
\end{table}

\textbf{Key Findings:}

\begin{enumerate}
  \item \textbf{Impact of Caption Quality:} The detailed caption strategy (v2) significantly outperforms generic captions (v1), with LPIPS improvement of 45\% ($0.284 \to 0.156$) and FID improvement of 56\% ($42.3 \to 18.7$). This demonstrates that rich, descriptive captions substantially enhance both identity preservation and generation quality.
  
  \item \textbf{Practical Trade-off:} Version 3 (general captions) provides a practical middle ground, achieving 29\% LPIPS improvement over v1 and 37\% FID improvement, while requiring less annotation effort than v2.
  
  \item \textbf{Identity Preservation:} Only v2 achieves excellent LPIPS score ($< 0.15$), indicating that generic identifiers alone are insufficient for faithful identity preservation in celebrity personalization.
  
  \item \textbf{Generation Quality:} v2's FID of 18.7 is well within the ``good'' range ($ < 30$), suggesting diverse, high-quality generation across varied prompts.
\end{enumerate}

\subsubsection{Qualitative Analysis}

Visual inspection of generated images reveals:

\begin{itemize}
  \item \textbf{v1 (Generic captions):} Generated images show recognizable subject characteristics but with inconsistent facial features, pose variations, and occasional identity drift across prompts.
  
  \item \textbf{v2 (Detailed captions):} Generated images exhibit strong identity consistency across diverse scenes and poses. Fine details (hair style, facial features) are well-preserved. Models respond appropriately to descriptive prompts (e.g., ``smiling'', ``wearing glasses'').
  
  \item \textbf{v3 (General captions):} Intermediate results with reasonable identity consistency and good responsiveness to prompt variations, though less precise than v2.
\end{itemize}

\subsubsection{Checkpoint Selection}

Fine-tuning checkpoints saved at epochs 6, 8, 10, 12 revealed that epoch 10 typically yielded optimal performance. Early epochs (6-8) showed underfitting, while later epochs (12+) occasionally showed signs of overfitting with reduced generation diversity.

\section{Conclusion and Future Work}

\subsection{Summary of Findings}

This project successfully demonstrates a PyTorch-based implementation of LoRA fine-tuning for Stable Diffusion personalization, achieving results comparable to UI-based tools (Kohya, A1111) while providing full code transparency and extensibility.

Key conclusions from our experiments:

\begin{enumerate}
  \item \textbf{Caption Quality is Critical:} Detailed, descriptive captions significantly outperform generic fixed identifiers. A 45\% improvement in LPIPS ($0.284 \to 0.156$) and 56\% in FID ($42.3 \to 18.7$) demonstrates that investing in high-quality annotations pays substantial dividends.
  
  \item \textbf{Practical Trade-offs Exist:} While detailed captions provide the best results, general captions offer reasonable performance (LPIPS 0.201, FID 26.5) with substantially less annotation effort, making them suitable for resource-constrained scenarios.
  
  \item \textbf{PyTorch Implementation Feasibility:} Our modular PyTorch codebase achieves performance parity with community tools, validating the approach as a viable research-friendly alternative to UI-based solutions.
  
  \item \textbf{Checkpoint Optimization:} Mid-training checkpoints (epoch 10) typically yield optimal balance between learning and generalization, suggesting that fixed training schedules may be suboptimal and adaptive selection strategies could improve results.
\end{enumerate}

\subsection{Contributions}

\begin{itemize}
  \item An open-source, reproducible PyTorch implementation of Stable Diffusion LoRA fine-tuning
  \item Systematic evaluation of caption quality impact on personalized generation
  \item Integration of LPIPS and FID metrics for quantitative evaluation
  \item Modular, extensible codebase suitable for research and experimentation
\end{itemize}

\subsection{Future Work}

Promising directions for future research include:

\begin{itemize}
  \item \textbf{Advanced Caption Strategies:} Investigate hierarchical captions, automated caption generation, or weakly-supervised annotation approaches to reduce labeling burden while maintaining quality.
  
  \item \textbf{Hybrid Fine-Tuning:} Combine LoRA with other adaptation techniques (e.g., textual inversion, DreamBooth-style class preservation) to achieve even better identity preservation.
  
  \item \textbf{Multi-Subject Personalization:} Extend to scenarios with multiple subjects in a single model, studying cross-subject interference and mitigation strategies.
  
  \item \textbf{Generative Metrics Analysis:} Deeper investigation of LPIPS vs. FID trade-offs; explore additional metrics like CelebA-attribute preservation scores for celebrity applications.
  
  \item \textbf{Computational Efficiency:} Study inference optimization techniques (quantization, distillation) to enable deployment of personalized models on resource-constrained devices.
  
  \item \textbf{Real-Time Adaptation:} Develop techniques for on-the-fly LoRA fine-tuning with streaming data, enabling personalized generation with minimal setup.
  
  \item \textbf{Cross-Domain Transfer:} Evaluate whether LoRA weights trained on one subject transfer to similar subjects, potentially reducing per-subject fine-tuning cost.
\end{itemize}

\newpage

\printbibliography
\end{document}
