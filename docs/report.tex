\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[tc]{titlepic}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[none]{hyphenat}
\usepackage[
  backend=biber,
  style=ieee,
  sorting=none
    ]{biblatex}
\usepackage[
  a4paper,
  left=2.5cm,
  right=2.5cm,
  top=2.5cm,
  bottom=2.5cm
]{geometry}
\usepackage{indentfirst}

\addbibresource{./assets/literature.bib}
\begin{document}

% Title and author
\begin{titlepage}
  \begin{center}
    \vspace*{2cm}

    
    \includegraphics[width = 10cm]{assets/oth.png}

    \begin{center}
      {Ostbayerische Technische Hochschule Amberg-Weiden}\\
      {Department of Electrical Engineering, Media and Computer Science}
    \end{center}

    \vspace{1.5cm}
    
    {\fontsize{18}{30}\selectfont 
        \bfseries 
        PERSONALIZED IMAGE GENERATION WITH STABLE DIFFUSION: \\[4pt]
        PYTORCH LORA FINE-TUNING APPROACH \\[6pt]
    }

    \vspace{1.5cm}

    \vspace{2cm}

    {\small\ Student}

    \vspace{0.25cm}

    {\Large\bfseries Tran Thai Duc Hieu }
    \vspace{0.25cm}
    
    {\small t.tran@oth-aw.de }

    \vspace{1cm}

    {\small\ Supervised by}

    \vspace{0.25cm}

    {\Large\bfseries Prof. Dr. Tatyana Ivanovska}

    \vspace{2cm}

    
    \vfill

    {\large\today}
  \end{center}
\end{titlepage}


\newpage

\section{Introduction}

Recent advances in diffusion models, particularly Stable Diffusion \cite{rombach2022high}, have demonstrated remarkable capabilities in text-to-image generation. However, generating highly personalized images of specific subjects or celebrities remains challenging, as it requires the model to learn and preserve identity-specific features while remaining generalizable to diverse prompts and contexts.

A popular modern approache to this problem have relied on UI-based tools such as Kohya and A1111 WebUI, which abstract away the underlying PyTorch configuration details. While these tools offer user accessibility, they limit customization and reproducibility for research purposes.

This work presents a PyTorch-based implementation achieving personalized image generation performance comparable to existing community tools and tutorials. By implementing Low-Rank Adaptation (LoRA) fine-tuning from first principles, this study provides a transparent, modular, and extensible framework for personalization that facilitates reproducible research and systematic experimentation.

The objectives of this work are threefold:
\begin{enumerate}
  \item Provide a reproducible, open-source PyTorch implementation for Stable Diffusion personalization
  \item Systematically study the impact of caption structure and other hyperparameters on model performance
  \item Establish quantitative evaluation metrics (LPIPS and FID) for assessing generation quality and identity preservation
\end{enumerate}

\section{Related work}

\subsection{Diffusion models and Stable Diffusion}

Diffusion models have emerged as a powerful class of generative models, achieving state-of-the-art results in image synthesis \cite{ho2020denoising}. Stable Diffusion \cite{rombach2022high} builds upon this foundation by combining diffusion models with latent space representations, enabling efficient high-quality image generation. The architecture comprises three main components: a text encoder (CLIP) \cite{radford2021learning}, a UNet-based denoising network, and a variational autoencoder (VAE) \cite{kingma2013auto}.

\subsection{Low-rank adaptation (LoRA)}

Full model fine-tuning requires updating billions of parameters, which is computationally expensive and prone to overfitting on small datasets. Low-Rank Adaptation (LoRA) \cite{hu2021lora} addresses this by introducing trainable low-rank decomposition matrices into the network. Instead of updating the weight matrix $W$, LoRA decomposes the weight update as:

\[
\Delta W = BA^T
\]

where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{k \times r}$, with $r \ll \min(d, k)$ (typically $r=8$ or $r=16$). This dramatically reduces the number of trainable parameters while maintaining model expressiveness. LoRA is particularly effective for Stable Diffusion personalization, where only the attention layers in the UNet are typically adapted.

\subsection{Personalized image generation}

Personalized image generation aims to teach the model to recognize and generate specific subjects (e.g., a particular person or object). Common approaches include:

\begin{itemize}
  \item \textbf{Textual Inversion:} Learns a new token embedding to represent a subject \cite{gal2022textual}, requiring only 3-5 reference images but producing less flexible results.
  \item \textbf{DreamBooth:} Uses subject-specific identifiers combined with class-preservation loss to achieve high-fidelity personalization \cite{ruiz2023dreambooth}.
  \item \textbf{LoRA Fine-Tuning:} Adapts attention layers with low-rank updates, balancing efficiency and quality. This approach is implemented in popular community tools like Kohya and A1111 WebUI.
\end{itemize}

\subsection{Evaluation metrics}

Quantitatively evaluating personalized image generation requires metrics that capture both identity preservation and overall synthesis quality:

\begin{itemize}
  \item \textbf{LPIPS (Learned Perceptual Image Patch Similarity):} Measures perceptual similarity between generated and training reference images using features from a pre-trained deep network (typically AlexNet) \cite{zhang2018unreasonable}. Lower values indicate better identity preservation. Benchmark targets: $< 0.15$ (excellent), $0.15-0.25$ (good), $> 0.25$ (weak).
  \item \textbf{FID (Frechet Inception Distance):} Measures statistical distance between generated and reference image feature distributions using the Inception-v3 network \cite{heusel2017gans}. Lower values indicate better overall generation quality and diversity. Benchmark targets: $< 10$ (excellent), $10-30$ (good), $30-50$ (moderate), $> 50$ (poor).
\end{itemize}

\subsection{Community tools and motivation for PyTorch implementation}

UI-based tools such as Kohya and A1111 WebUI have enabled broader adoption of LoRA fine-tuning through user-friendly interfaces and pre-configured settings, and are accompanied by practical community guides detailing SD~1.5 workflows \cite{arbues2024lora,civitai2024celebrity}. However, these tools typically obscure implementation details, limiting research flexibility and code reproducibility. This work implements an equivalent PyTorch-based pipeline that follows established community configurations while providing fully interpretable, modular code suitable for research and controlled experimentation.

\section{Dataset preparation and experimental setup}

\subsection{Data collection and preprocessing}

A dataset comprising 151 images of a target subject (Jennie) was assembled to evaluate personalized generation capabilities. The dataset preparation process involved two primary steps:

\begin{enumerate}
  \item \textbf{Image collection:} High-quality reference images of the target subject were curated from public sources.
  
  \item \textbf{Image preprocessing:} All images were standardized to $512 \times 512$ pixels while preserving the original aspect ratio to maintain visual fidelity. Images exhibiting excessive occlusion or low perceptual quality were excluded.
  
\end{enumerate}

\subsection{Caption strategies and dataset versions}

A keyword-based captioning paradigm was employed across all experiments. This approach relies on learning unique identifiers and descriptive tokens to achieve personalization, contrasting with traditional fine-tuning that updates all model weights. To systematically evaluate the effect of caption structure on model performance, three dataset versions with varying levels of attribute annotation were created:

\begin{itemize}
  \item \textbf{Version 1 (v1):} Single unique trigger word ``J3NN13''. This identifier was deliberately chosen to be a non-dictionary word, ensuring the model associates this specific token exclusively with the target subject. All captions in this version consist solely of this trigger word, establishing a minimal baseline where the model must learn identity purely through visual association without additional contextual cues.
  
  \item \textbf{Version 2 (v3):} Detailed comma-separated keyword tags combining the trigger word ``J3NN13'' with multiple attribute descriptors: expressions (e.g., ``wink'', ``teeth-smiling''), camera framing (``close-up'', ``straight-look''), appearance details (``long hair'', ``strap top''), and context (``wall background'', ``bright''). Example: ``J3NN13, wink, close-up, straight-look, make-up, tongue-out, braiding hair''. This maximizes attribute diversity and provides rich contextual information alongside the trigger word.
  
  \item \textbf{Version 3 (v4):} Simplified comma-separated keywords with fewer attributes per image, focusing on essential identity and contextual features. Example: ``J3NN13, wink, close-up, make-up, tongue-out, braiding hair'' or ``J3NN13, close-up, wall background''. This balances attribute specificity with annotation simplicity.
\end{itemize}

\section{Experimental setup and implementation}

This section documents the iterative experimental methodology employed to optimize personalized image generation. Rather than presenting a single configuration, the evolution of the approach across multiple experiments is described, emphasizing key findings and design decisions. Detailed results for all experiments (excluding Experiment 0) are presented below.

\subsection{Initial baseline experiments (experiments 0)}

Following community tutorials \cite{arbues2024lora} recommending 1500-4500 training steps, we established initial baselines to understand training dynamics:


\begin{table}[H]
  \centering
  \caption{Experiment 0 configuration}
  \label{tab:exp0-config}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Parameter & Value \\
    \midrule
    Base Model & Stable Diffusion v1.5 (runwayml/stable-diffusion-v1-5) \\
    Dataset & 151 images with v1 captions \\
    LoRA Rank ($r$) & 32 \\
    LoRA Alpha ($\alpha$) & 16 \\
    Batch Size & 2 \\
    Training Epochs & 100 \\
    \bottomrule
  \end{tabular}
\end{table}

\textbf{Results:} Although the configuration follows established community guidelines from popular Kohya and Automatic1111 tutorials, the results did not achieve the expected performance. Visual quality was poor, with weak identity preservation and significant visual artifacts. This outcome indicated that the training duration was insufficient.

\subsection{Realistic base model experiments (experiments 1-3)}

Vanilla Stable Diffusion v1.5 lacks photorealistic capabilities necessary for high-quality celebrity personalization. Consequently, a community-fine-tuned model (\textbf{SG161222/Realistic\_Vision\_V5.1\_noVAE}) \cite{sg161222_realistic_vision} specializing in realistic portrait generation was employed for all subsequent experiments.

Table \ref{tab:common-config} specifies the common training configuration used consistently across experiments 1--3. For each experiment, dataset versions 1, 2, and 3 respectively were used to investigate the effect of caption structure on model performance.



\begin{table}[H]
  \centering
  \caption{Common training configuration for experiments 1--3}
  \label{tab:common-config}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Parameter & Value \\
    \midrule
    Base Model & SG161222/Realistic\_Vision\_V5.1\_noVAE \\
    Dataset Size & 151 images \\
    LoRA Rank ($r$) & 32 \\
    LoRA Alpha ($\alpha$) & 16 \\
    LoRA Dropout & 0.1 \\
    Target Modules & to\_q, to\_k, to\_v, to\_out.0 \\
    Batch Size & 2 \\
    Learning Rate & $5 \times 10^{-5}$ \\
    Training Epochs & 300 \\
    \midrule
    Inference model & SG161222/Realistic\_Vision\_V5.1\_noVAE \\
    
    \bottomrule
  \end{tabular}
\end{table}

However, for each experiment, we use dataset versions 1 to 3 respectively.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{assets/exp1_preview.png}
    \caption{Experiment 1 samples}
    \label{fig:exp1_preview}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{assets/exp2_preview.png}
    \caption{Experiment 2 samples}
    \label{fig:exp2_preview}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{assets/exp3_preview.png}
    \caption{Experiment 3 samples}
    \label{fig:exp3_preview}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{assets/exp4_preview.png}
    \caption{Experiment 4 samples}
    \label{fig:exp4_preview}
\end{figure}
\subsection{Transferred inference experiment (experiment 4)}

Experiments 1--3 did not reach the quality reported in community tutorials. To assess the effect of the inference model, we separate training from inference: LoRA weights are trained on Stable Diffusion v1.5 and later applied to the Realistic Vision model at inference time. This cross-model transfer evaluates whether features learned during training can improve generation quality when used with a higher-fidelity, photorealistic base model.

\begin{table}[H]
  \centering
  \caption{Training configuration for experiment 4: cross-model transfer}
  \label{tab:exp4-config}
  \begin{tabular}{@{}lc@{}}
    \toprule
    Parameter & Value \\
    \midrule
    Base Model (Training) & runwayml/stable-diffusion-v1-5 \\
    Dataset Size & 151 images \\
    LoRA Rank ($r$) & 32 \\
    LoRA Alpha ($\alpha$) & 16 \\
    LoRA Dropout & 0.1 \\
    Target Modules & to\_q, to\_k, to\_v, to\_out.0 \\
    Batch Size & 2 \\
    Learning Rate & $5 \times 10^{-5}$ \\
    Training Epochs & 300 \\
    \midrule
    Base Model (Inference) & SG161222/Realistic\_Vision\_V5.1\_noVAE \\
    
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Comparison across all experiments}

Figure~\ref{fig:metrics} summarizes LPIPS and FID across all experiments. 
Visual inspection indicates that Experiments~1 and~4 produce the most natural samples. 
Quantitatively, Table~\ref{tab:all-experiments} shows Experiment~4 achieving the lowest LPIPS of \textbf{0.752}.
An additional cross-model settings reported in Table~\ref{tab:additional-experiments}. 
Further analysis is provided in the Discussion section.



\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{assets/_metrics.png}
    \caption{LPIPS and FID metrics across all experiments}
    \label{fig:metrics}
\end{figure}


\begin{table}[H]
  \centering
  \caption{Summary of all experiments}
  \label{tab:all-experiments}
  \begin{tabular}{@{}lllllcc@{}}
    \toprule
    Run & Base model & Infer. model & Caption type & Guidance & \textbf{LPIPS} & \textbf{FID} \\
    \midrule
    Experiment 1 & R.Vision & R.Vision & v1 & 4.0 & 0.809 & 192.3 \\
    Experiment 1 & R.Vision & R.Vision & v1 & 6.0 & 0.797 & 195.4 \\
    Experiment 1 & R.Vision & R.Vision & v1 & 7.5 & 0.812 & 196.4 \\
    \midrule
    Experiment 2 & R.Vision & R.Vision & v2 & 4.0 & 0.786 & 199.6 \\
    Experiment 2 & R.Vision & R.Vision & v2 & 6.0 & 0.782 & 194.7 \\
    Experiment 2 & R.Vision & R.Vision & v2 & 7.5 & 0.764 & 191.9 \\
    \midrule
    Experiment 3 & R.Vision & R.Vision & v3 & 4.0 & 0.765 & \textbf{182.2} \\
    Experiment 3 & R.Vision & R.Vision & v3 & 6.0 & 0.765 & 186.6 \\
    Experiment 3 & R.Vision & R.Vision & v3 & 7.5 & 0.782 & 182.5 \\
    \midrule
    Experiment 4 & R.Vision & SD v1.5 & v3 & 4.0 & \textbf{0.752} & 198.4 \\
    Experiment 4 & R.Vision & SD v1.5 & v3 & 6.0 & 0.754 & 207.3 \\
    Experiment 4 & R.Vision & SD v1.5 & v3 & 7.5 & 0.753 & 215.9 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Additional experiments}
  \label{tab:additional-experiments}
  \begin{tabular}{@{}lllllcc@{}}
    \toprule
    Run & Base model & Infer. model & Caption type & Guidance & LPIPS & FID \\
    \midrule
    Experiment 1 & R.Vision & SD v1.5 & v1 & 4.0 & 0.816 & 255.5 \\
    Experiment 1 & R.Vision & SD v1.5 & v1 & 6.0 & 0.835 & 217.7 \\
    Experiment 1 & R.Vision & SD v1.5 & v1 & 7.5 & 0.828 & 202.5 \\
    \midrule
    Experiment 2 & R.Vision & SD v1.5 & v3 & 4.0 & 0.806 & 299.5 \\
    Experiment 2 & R.Vision & SD v1.5 & v3 & 6.0 & 0.818 & 242.2 \\
    Experiment 2 & R.Vision & SD v1.5 & v3 & 7.5 & 0.800 & 224.1 \\
    \midrule
    Experiment 3 & R.Vision & SD v1.5 & v4 & 4.0 & 0.808 & 243.5 \\
    Experiment 3 & R.Vision & SD v1.5 & v4 & 6.0 & 0.813 & 214.6 \\
    Experiment 3 & R.Vision & SD v1.5 & v4 & 7.5 & 0.793 & 202.3 \\
    \midrule
    Experiment 4 & SD v1.5 & SD v1.5 & v4 & 4.0 & 0.787 & 181.6 \\
    Experiment 4 & SD v1.5 & SD v1.5 & v4 & 6.0 & 0.758 & 196.2 \\
    Experiment 4 & SD v1.5 & SD v1.5 & v4 & 7.5 & 0.772 & 184.2 \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Evaluation protocol}

For each experiment, the following evaluation procedure was applied:

\begin{enumerate}
  \item \textbf{Inference prompt design}: Prompts were given in a way that it includes words used during training to check and also words that were not used during training to test the learning ability and the generalization of the model.

  Example prompts:
  \begin{itemize}
    \item ``J3NN13, close-up selfie in a room background''
    \item ``J3NN13, HD quality, RAW photo, good lighting, selfie in a gym background''
  \end{itemize}

  \item \textbf{Image generation}: 5 distinct text prompts were used to generate 4 images per prompt, yielding 20 generated images total. Inference parameters were set to 60 diffusion steps with guidance scale $\lambda \in \{4.0, 6.0, 7.5\}$.
  \item \textbf{LPIPS computation}: Learned Perceptual Image Patch Similarity was computed by comparing each generated image to randomly selected reference images from the training set using the AlexNet feature extractor.
  \item \textbf{FID computation}: Fr√©chet Inception Distance was calculated between feature distributions of the 20 generated images and 151 reference training images using Inception-v3.
  \item \textbf{Qualitative analysis}: Generated samples were visually assessed for identity consistency, artifact presence, and adherence to text prompts.
\end{enumerate}

\textbf{Note:} The relatively small number of generated images (20 total) for evaluation may introduce variance in metric estimates. Future work should employ larger evaluation sets (100+ images) for more robust and statistically reliable metric computation.


\subsection{Discussion}

\subsection{Key findings}

\begin{enumerate}
  \item \textbf{Training duration:} \\
  
  Experiment 0 with 100 epochs demonstrated insufficient convergence. Extending training to 300 epochs (approximately 22,800 training steps with batch size 2) proved necessary for model adaptation.

  If the generated images still exhibit poor quality (many artifacts, object edges is too sharp, background inconsistencies or it feels like a painting) or weak identity preservation, consider increasing the number of training epochs further (e.g., 400-500 epochs)
  
  The samples from Experiment 2 and Experiment 3 show low quality at 300 epochs, indicating that even longer training may be required for optimal results.
    
  \item \textbf{Caption strategy trade-offs:} 
  \begin{itemize}
    \item Version 1 (single trigger word): \\
    Minimal information, requiring purely visual identity learning.
    Surprisingly, this simple approach can achieve reasonably good results with sufficient training, see Fig. \ref{fig:exp1_best}.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.75\linewidth]{assets/exp1_best.png}
        \caption{Best samples from Experiment 1}
        \label{fig:exp1_best}
    \end{figure}

    The weakness of this approach is that it can only generate good results of close-up portraits or features that are very similar to the training images. The model lacks contextual understanding, leading to failures when generating diverse poses, backgrounds, or full-body shots. \\
    \item Version 2 (detailed attribute tags): 

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.75\linewidth]{assets/exp2_best.png}
        \caption{Best samples from Experiment 2}
        \label{fig:exp2_best}
    \end{figure}


    \item Version 3 (simplified attribute tags): 
    Optimal balance between annotation simplicity and attribute diversity. Good results already achieve in Experiment 4, see Fig. \ref{fig:exp4_best}.

  \end{itemize}

  \item \textbf{Cross-model transfer:} \\
  Experiment 4 demonstrated that training LoRA weights on Stable Diffusion v1.5 and applying them to the Realistic Vision model during inference can enhance identity preservation (lowest LPIPS).
  The general base model (SD v1.5) appears to force the LoRA to learn stronger identity features and information, which then benefit from the photorealistic capabilities of the Realistic Vision model at inference time.
  But this improvement in LPIPS comes at the cost of overall image quality (higher FID), likely due to domain mismatch between training and inference models.

  \begin{figure}[H]
      \centering
      \includegraphics[width=0.75\linewidth]{assets/exp4_best.png}
      \caption{Best samples from Experiment 4}
      \label{fig:exp4_best}
  \end{figure}

  Notice that the style of Realistic Vision model (the overall theme and lighting is dark and moody) is still dominant in the generated images


  \item \textbf{Inference prompts:} \\
  Carefully chosen keywords in inference prompts significantly influence generation quality. Including descriptive words like ``HD quality'', ``good lighting'', ``RAW photo'', or ``4K'' noticeably improves realism and quality. A practical tip: investigate the training data and captions used to train your base model. Reusing similar keywords and phrasing patterns from the model's training set can further enhance results, as the model is more familiar with these descriptors.

  \item \textbf{Guidance scale:} \\
  A guidance scale is a hyperparameter that controls the trade-off between how closely the generated image follows the text prompt versus how much creativity/diversity is allowed. \\
  Higher guidance scales force the model to obey the prompt more strictly, which can improve relevance but may also introduce unwanted artifacts if words in the prompt are ambiguous or unfamiliar to the model during training. \\

  This bad effect of high guidance scale can be seen clearly in Experiment 3 and Experiment 4, where increasing the guidance scale to 7.5 leads to more artifacts, distorted body parts and unnarual textures and skin tones.

  \item \textbf{Negative prompts:} \\
  Carefully crafted negative prompts offer an effective way to enhance generation quality and structural correctness. By specifying undesired features, the model learns to actively avoid them, resulting in improved image structure and coherence. 
  
  Across all experiments, asymmetric or misaligned facial features appeared regularly in generated images. Adding negative prompts like ``asymmetric eyes'', ``crossed eyes'', or ``distorted face'' consistently reduced these artifacts. Similarly, negative prompts addressing anatomical issues (e.g., ``wrong anatomy'', ``malformed hands'', ``distorted body parts'') noticeably improved overall generation quality. This approach complements positive prompts and guidance scale adjustments, providing an additional mechanism for fine-tuning output quality.

\end{enumerate}

\subsection{Limitations and future work}
While this work establishes a solid foundation for personalized image generation using LoRA fine-tuning, several limitations and avenues for future research remain:
\begin{itemize}
  \item \textbf{Natural sentence captions vs keyword-based captions:} \\
  The current approach relies exclusively on keyword-based captions. Future research should explore natural language descriptions (e.g., ``J3NN13 is smiling at the camera with long black hair'') to assess whether more contextual, grammatically complete captions improve identity learning and generation flexibility. Natural sentences may provide richer semantic relationships between attributes.
  
  \item \textbf{Extended training for complex caption structures:} \\
  Experiments 2 and 3, which employed more detailed caption versions, showed suboptimal results at 300 epochs. Increasing training duration (e.g., 500--1000 epochs) for datasets with comprehensive attribute annotations may allow the model to fully leverage the additional contextual information and achieve better identity preservation with enhanced controllability.
  
  \item \textbf{Facial features and hand generation:} \\
  Despite negative prompt adjustments, eyes and hands remain challenging to generate accurately. Asymmetric eyes, misaligned pupils, and malformed fingers appeared across experiments. Future work should investigate targeted training techniques (e.g., weighted loss on facial regions, specialized hand datasets) or post-processing refinement methods to address these persistent anatomical issues.

  \item \textbf{Post-processing and refinement workflows:} \\
  Integrating post-processing techniques such as face restoration models (e.g., CodeFormer, GFPGAN), inpainting tools, or ControlNet-based refinement could systematically improve generation quality. Establishing automated pipelines that combine LoRA generation with targeted post-processing steps would enhance practical applicability while maintaining identity consistency.
  
\end{itemize}

\newpage

\printbibliography
\end{document}
